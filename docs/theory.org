* Notes on theory

This document serves to have an idea of the theory behind the project and why certain algorithms have been developed. 

The project is mainly based on two papers: 
    1) "Gromov-Wasserstein Averaging of Kernel and Distance Matrices", by G. Peyré, M. Cuturi and J. Solomon
    2) "Gromov-Wasserstein Learning for Graph Matching and Node Embedding", by H. Xu, D. Luo, H. Zha and L. Carin.

** Gromov-Wasserstein Averaging of Kernel and Distance Matrices

First of all, let us fix some notation: in the problems presented in this paper, we don't really see the structure of a graph, 
    indeed we will work with:
        i) a vector \mu of length n \in N, where n could be thougth as the number of nodes and the i-th element of \mu is the 
            importance of the i-th node. We will always assume \mu to be non-negative and sum 1, and it is initialized to be 
            the uniform distribution if not specified
        ii) a square matrix C of dimensione n, whose element C_ij represents how far the i-th node is from the j-th. It is 
            improperly called *distance matrix*, because it is not really a distance, in the sense that we don't require it to be 
            symmetric, non-negative and to satisfy the traingular inequality. Most properly we will call it *dissimilarity matrix*.
            Anyway, in many cases it will be induced by a function: to understand this, imagine that the nodes of the graph are point 
            {x_i} of R^2 (or R^d more generally) and that k:R^2 x R^2 \to R is any function, then we define C_ij = k(x_i,x_j).

So, the problems presented in this first part can be defined in the very general setting in which we have a couple (C,\mu), as above,
but mostly C will be defined using a function k as shown above.

We will call such a couple a *metric measure space*.


*** Gromov Wasserstein discrepancy

Given a couple of metric measure spaces (C,\mu) and (C',\mu'), the authors define the discrepancy between them using the definition 
given by F. Memoli in "Gromov–Wasserstein Distances and the Metric Approach to Object Matching".

#SCRIVERE LA FORMULA?

The idea of such definition is the following: find a multivariate matching T (also called transport plan,i.e. a function that can 
assigns more than a value to a single point) between the nodes, in such a way that the weigth \mu and \mu' are preserved by this 
matching and C_i,j is not "much" different from C_T(i),T(j). 

In other words, as all the possible matchings vary, they want to minimize the sum of all L(C_i,j ; C_T(i),T(j)), where L is a fixed
loss function, usually initialized to be  
    i) either the common square distance L(x,y) = |x-y|^2
    ii) or the so called Kullback-Leibler divergence L(x,y) = KL(a|b) = a*log(a/b)-a+b


**** Entropic Gromov-Wasserstein discrepancy

Computationally, the minimization problem above is not so efficient to solve, thus the authors consider a sligth modification of it,
adding an *entropic convex*, that must be thought as a regularizing term:

#SCRIVERE LA FORMULA?

so that the solution of this problem (and then of the original one) can be evaluated using the projected gradient descent algorithm,
which in a particular choice of some constants (i.e. epsilon and tau, we will be in this hypothesis) it can be substituted by the 
Sinkhorn algorithm, which in time is very a efficient algorithm. 
[Here I'm referring to eqaution (8) and Proposition 2, how many details should we put of it?]


*** Gromov-Wasserstein barycenters

Using the entropic Gromov-Wasserstein discrepancy, given a finite collection of metric measure spaces (C_s,\mu_s) and a vector of
weigths (\lambda_s), the authors define the barycenter problem

#SCRIVERE LA FORMULA?

and this is the problem on which we are concentrating, writing in Julia the algorithm they proposed using the theory above.



** Gromov-Wasserstein Learning for Graph Matching and Node Embedding


