* Notes on theory

This document serves to have an idea of the theory behind the project and why certain algorithms have been developed. 

The project is mainly based on two papers: 
    1) "Gromov-Wasserstein Averaging of Kernel and Distance Matrices", by G. Peyré, M. Cuturi and J. Solomon
    2) "Gromov-Wasserstein Learning for Graph Matching and Node Embedding", by H. Xu, D. Luo, H. Zha and L. Carin.

** Gromov-Wasserstein Averaging of Kernel and Distance Matrices

First of all, let us fix some notation: in the problems presented in this paper, we don't really see the structure of a graph, 
    indeed we will work with:
        i) a vector \mu of length n \in N, where n could be thougth as the number of nodes and the i-th element of \mu is the 
            importance of the i-th node. We will always assume \mu to be non-negative and sum 1, and it is initialized to be 
            the uniform distribution if not specified
        ii) a square matrix C of dimensione n, whose element C_ij represents how far the i-th node is from the j-th. It is 
            improperly called *distance matrix*, because it is not really a distance, in the sense that we don't require it to be 
            symmetric, non-negative and to satisfy the traingular inequality. Most properly we will call it *dissimilarity matrix*.
            Anyway, in many cases it will be induced by what we will call an *embedding*: to understand this, imagine that the nodes 
            of the graph are point {x_i} of R^2 (or R^d more generally) and that k:R^2 x R^2 \to R is any function, then we define C_ij = k(x_i,x_j).

So, the problems presented in this first part can be defined in the very general setting in which we have a couple (C,\mu), as above,
but mostly C will be defined using a function k as shown above.

We will call such a couple a *metric measure space*.


*** Gromov Wasserstein discrepancy

Given a couple of metric measure spaces $(C,\mu)$ and $(C^*,\mu^*)$, the authors define the discrepancy between them using the definition 
given by F. Memoli in "Gromov–Wasserstein Distances and the Metric Approach to Object Matching":

$$\operatorname{GW}(C,\mu;C^*,\mu^*) = \min_{T \in \Gamma(\mu,\mu^*)} \sum_{i,j,k,l} L(C_{ik},C^*_{jl})T_{ij}T_{kl}, $$

where $T \in \Gamma(\mu,\mu^*)$ means that $\sum_{j} T_{ij} = \mu_i$ and $\sum_i T_{ij} = \mu^*_j$. 

The idea of such definition is the following: find a multivariate matching T (also called transport plan,i.e. a function that can 
assigns more than a value to a single point) between the nodes, in such a way that the weigth \mu and \mu' are preserved by this 
matching and $C_{i,j}$ is not "much" different from $C_{T(i),T(j)}$. 

In other words, as all the possible matchings vary, they want to minimize the sum of all $L(C_i,j ; C_T(i),T(j))$, where L is a fixed
loss function, usually initialized to be  
    i) either the common square distance $L(x,y) = |x-y|^2$
    ii) or the so called Kullback-Leibler divergence $L(x,y) = KL(a|b) = a\log(a/b)-a+b$


**** Entropic Gromov-Wasserstein discrepancy

Computationally, the minimization problem above is not so efficient to solve, thus the authors consider a sligth modification of it,
adding an *entropic convex*, that must be thought as a regularizing term:

\begin{equation}
\operatorname{GW}_{\varepsilon}(C,\mu;C^*,\mu^*) = \min_{T \in \Gamma(\mu,\mu^*)} \sum_{i,j,k,l} L(C_{ik},C^*_{jl})T_{ij}T_{kl} -
\varepsilon H(T), 
\end{equation}

where $H(T)=-\sum_{i,j}T_{ij}(\log(T_{ij}-1))$.


so that the solution of this problem (and then of the original one) can be evaluated using the projected gradient descent algorithm,
which in a particular choice of some constants (i.e. epsilon and tau, we will be in this hypothesis) it can be substituted by the 
Sinkhorn algorithm, which in time is very a efficient algorithm. 
[Here I'm referring to eqaution (8) and Proposition 2.]


*** Gromov-Wasserstein barycenters

Using the entropic Gromov-Wasserstein discrepancy, given a finite collection of metric measure spaces $(C_s,\mu_s)_{s=1}^S$ and a vector of
weigths $(\lambda_s)$, the authors define the barycenter problem as

\begin{equation}
\min_{C \in \mathbb{R}^{N \times N}} \sum_{s=1}^S\lambda_s \operatorname{GW}_{\varepsilon}(C,\mu;C_s,\mu_s),
\end{equation}

where we are assuming that the weight $\mu$ of the barycenter is fixed in advance.
This is the problem on which we are concentrating, writing in Julia the algorithm they proposed using the theory above.
[I'm referring to Algorithm 1.]


** Gromov-Wasserstein Learning for Graph Matching and Node Embedding

The paper by Cuturi, Peyré and Solomon had some limitations in the way it's defined the discrepancy of two graphs: given two metric
measure spaces $(C,\mu)$ and $(C^*,\mu^*)$, we would like to learn if they come from an embedding or not, or, anyway, how far they 
are from an embedding.

So, let's fix the embedding space: fix a dimension $D\in\mathbb{N}$ and a function $k:\mathbb{R}^D \times \mathbb{R}^D \to \mathbb{R}$.
Given $(C,\mu)$ as above of dimension n, an honest problem would be to find a collection $(x_k)_{k=1}^n$ of elements of $\mathbb{R}^D$
s.t. the metric measure space define by this collection (as an embedding, you can find the definition at the beginning of the presentation
of the other paper) is not "so different" from $(C,\mu)$.


The aim of this work, is to find both two embeddings $(x_k)_k$ and $(x_l^*)_l$ which are, respectively, close to $(C,\mu)$ and
$(C^*,\mu^*)$ s.t. they minimize a suitable modification of Gromov-Wasserstein discrepancy, with some loss function. 

The cost is not exactly the GW-discrepancy because, of course, it should care about the embedding. The authors decided to define the 
following problem:

#SCRIVERE FORMULA (2)

where $C(X) = (1-\alpha)C + \alpha K(X,X)$ and $K(X,X)$ is exactly the matrix given by the embedding. The parameter $\alpha\in[0,1]$ is
telling us how much to care about the embedding. We will see that in the algorithm it will vary in such a way that, in an iterative
process, at any step we will increment the "importance" that we are giving to the embedding: this will be exactly the idea of obtaining 
at the same time the two embeddings and a matching $T$ that minimizes their discrepancy.
  

