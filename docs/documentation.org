* Documentation 

In this document, we describe the algorithms that we implemented to compute the Gromov-Wasserstein barycenters we talk about in theory.org.


** MetricMeasureSpace.jl
In this file we implement the struct we talk about in implementation.org.


** RepeatUntilConvergence 
In this file, we defined a structure that can repeat a function until convergence. It will be more clear in the following.

*** abstract type BaseRepeatUntilConvergence{T}
It is a parametric abstract type that has, as subtypes, the concrete parametric types 
    - RepeatUntilConvergence
    - LargeMemoryRepeatUntilConvergence 
    - SingleValueRepeatUntilConvergence 

*** mutable struct RepeatUntilConvergence{T}
It is a parametric struct defined in the following way: 
 
#+begin_src julia :results output
mutable struct RepeatUntilConvergence{T} <: BaseRepeatUntilConvergence{T}
    update_func::Function
    has_converged::Function
    history::CircularBuffer{T}
    init_vals::T 
end
#+end_src

**** T 
T is the type on which a type RepeatUntilConvergence depends on: the idea is that we have an update function that takes some input of type T and 
returns something of type T, so that we can iterate this function.

**** update_func
The update function is the function that we need to iterate on something of type T.

**** has_converged
This function is a criterion for convergence that returns a Bool type. It checks if we have to stop the execution, possibly using the history 
field.

**** history
The history contains the output that we need to store to check the stop criterion. It has been organized in a CircularBuffer{T} type because, as
we said above, all the outputs of the update function are of type T and here we can store only that elements, and we didn't choose a Vector{T}
because maybe is not necessary to store all the outputs: for example the stop criterion could involve just the last two output, so that we need 
to have a CircularBuffer{T} of length 2.

**** init_vals
It is the initial value of the iterative process, so it is the first value that is updated, which means that it must be of type T.

**** inner constructor
Arguments: an update function, an has_converged function and a Int memory_size.
It returns an element of type RepeatUntilConvergence{T} initializing the memory size of the CircularBuffer{T} according to memory_size. 
Before doing that, it checks that the update function has a method for type T and returns a type T, and it check that the has_converged function
has a method for type Vector{T} and returns a Bool type, otherwise it raises an error. 
Achtung: it doesn't initialize init_vals. 

*** mutable struct LargeMamoryRepeatUntilConvergence{T}
It is a parametric struct defined in the following way: 
 
#+begin_src julia :results output
mutable struct RepeatUntilConvergence{T} <: BaseRepeatUntilConvergence{T}
    update_func::Function
    has_converged::Function
    history::Vector{T}
    init_vals::T 
end
#+end_src

It is exactly as the other, except that the history is a Vector{T}. It could be used when we need to use all the outputs the execution produced.
We didn't implemented it, because we never use it.

*** mutable struct SingleValueRepeatUntilConvergence{T}
It is a parametric struct defined in the following way: 
 
#+begin_src julia :results output
mutable struct RepeatUntilConvergence{T} <: BaseRepeatUntilConvergence{T}
    update_func::Function
    has_converged::Function
    history::T
    init_vals::T 
end
#+end_src
 
It is exactly as the others, except that the history is an element of type T. It could be used when we need to store only the last output (we
could use also the classic RepeatUntilConvergence{T} with memory_size=1, but this is more efficient in this case).
We didn't implemented it, because we never use it.

*** Function update_history!
Arguments (first method): an element of type either RepeatUntilConvergence{T} or LargeMemoryRepeatUntilConvergence{T}, and an element iter_result
of type T (usually this is the result of an execution of the update function).
Output: the function pushes the iter_result at the end of the history.

Arguments (second method): an element of type SingleValueRepeatUntilConvergence{T} and an element iter_result
of type T (usually this is the result of an execution of the update function).
Output: it lets the history field to be iter_result.

*** Function has_converged_wrapper
Arguments (first method): an element of type RepeatUntilConvergence{T}.
Output: it converts the CircularBuffer{T} into a Vector{T} and then it returns true or false according to the has_converged function applied on
the converted history.

Arguments (second method): an element of type either LargeMemoryRepeatUntilConvergence{T} or SingleValueRepeatUntilConvergence{T}.
Output: it returns true or false according to the has_converged function applied on
the converted history.

*** Function execute! 
It just execute the process of a BaseRepeatUntilConvergence{T} element, given an initial value.


** SinkhornKnopp.jl
The Sinkhorn-Knopp algorithm is an iterative algorithm which computes an (approximate) solution of the following minimum problem: 
$$\min \; C \cdot \T + \varpesilon KL(T),$$
where the minimum is taken over all the transport plans between two fixed marginal distributions p and q, $\varepsilon$ is a fixed 
(small) constant and KL is the Kullback-Leibler divergence.

*** struct data_SK
The struct "data_SK" contains all the necessary to compute a single update of the Sinkhorn-Knopp algorithm.

#+begin_src julia :results output
struct data_SK:
    K::Matrix{Float64} 
    p::Vector{Float64} 
    q::Vector{Float64} 
    T::Matrix{Float64}       
    a::Vector{Float64}       
    b::Vector{Float64}
    inner_constructor(K,p,q,T)   
end
#+end_src

**** K 
The matrix K is the element-wise exponentiation of C/epsilon, so it must be used only with this setting.


**** p and q 
These two vectors are the marginal distributions, so they must be non-negative and with sum 1.

**** T 
T is a feasible transport plan between p and q.

**** a and b 
They are the vectors that are updated by the Sinkhorn algorithm.

**** inner_constructor
It takes just K, p, q and T. It just checks that the dimensions of this object are correct, and then it built an element of type data_SK with
K, p, q, T, a \= constant vector with sum 1 (it actually could be any initialization, we just decided for this one) and $b \= \frac{q}{K^T*a}$
     
*** Function update_SK 
Arguments: an element of type data_SK. 
Output: it computes a single iteration of the Sinkhorn algorithm updating a, b and T in the following way: 
     $$a = \frac{p}{K*b}, \quad b = \frac{q}{K^T*a}, \quad T = diag(a)*K*diag(b)$$

*** Function compute_marginals 
Arguments: a squared  matrix.
Output: two vector, which are obtained summing all the rows and all the columns (one must think the matrix as the element T of a data_SK and 
the hope is that this two vectors are "similar" to p and q).

*** Function stop_SK_T
Arguments: a vector history, of size at least 2, of elements of type data_SK and a float tol, which is the tolerance. 
Output: it returns true if the 1-norm between history[end].T and history[end-1].T is less than tol, otherwise it return false.

*** Function stop_SK_ab_old
Arguments: a vector history, of size at least 1, of elements of type data_SK and a float tol, which is the tolerance. 
Output: it computes the marginals $\mu$ and $\nu$ of history[end].T and it returns true if both the 1-norm of history[end].p-$\mu$ and 
history[end-1].q-$\nu$ are less than tol, otherwise it return false.

*** Function stop_SK_ab_new
Arguments: a vector history, of size at least 1, of elements of type data_SK and a float tol, which is the tolerance. 
Output: it does exactly the same of stop_SK_ab_old without using directly the T filed of the struct, but recomputing it using a, b and K fields.

*** Function stop_SK
Arguments: a vector history, of size at least 1, of elements of type data_SK and a float tol, which is the tolerance. 
Output: it returns true if both stop_SK_T and stop_SK_ab_new return true on the same arguments, otherwise it returns false (this is the
most precise stop criterion, since it checks both the difference betwwen the updating of the transport and how much the marginal distributions
are different from the ones we want).


** loss.jl
In this file we built a simple struct "loss" to make more compact the syntax in the future algorithms concerning the chosen of the loss function.

The theory tells us that the Gromov-Wasserstein distance between two finite metric measure spaces $(C,\mu)$ and $(D,\nu)$ is given by 
$$GW((C,\mu),(D,\nu)) = \min_T \sum_{i,j,k,l} L(C_{ik},D_{jl})T_{ij}T_{kl},$$
where the infimum is taken over all the transport plans T between the marginals $\mu$ and $\nu$. In a more compact way, we will write the
expression above as $\langle L\otimes T , T\rangle$, where the matrix $L(C,D)\otimes T$ is defined as 
$$(L(C,D) \otimes T)_{kl} = L(L(C_{ik},D_{jl})T_{ij}).$$

So, to define the Gromov-Wasserstein distance, we need a function $L:\mathbb{R} \to \mathbb{R}$, called loss function. The only admissible
functions for this work are the L2 loss and the KL loss, defined as 
$$L2(a,b) = (a-b)^2,\quad KL(a,b) = a\log(a/b) -a +b.$$

In general, for this algorithm, one can consider loss functions that can be written as $L(a,b) = f_1(a) + f_2(b) - h_1(a)h_2(b)$ (note that 
L2 and KL can be written in this way). This form is important for the computation of the tensor product $L(C,D)\otimes T$, that can be computed 
using the following formula
$$L(C,D) \otimes T = f_1(C) *\mu * ones(n)^T + ones(m) *\nu * f_2(D)^T -h_1(C) *T * h_2(D)^T,$$
where n is the size of $(C,\mu)$, m is the size of $(D,\nu)$, the exponentiation to T is the transpose and the functions f1, f2, h1, h2 
are applied element-wise.

*** struct loss
It contains all the informations we talked above regarding a loss function.

#+begin_src julia :results output
struct Loss:
    string::String
    f1::Function
    f2::Function
    h1::Function
    h2::Function  
end
#+end_src

**** string
It contains the name of the loss function. The only admissible strings are "L2" and "KL", to distinguish when we use the Euclidean loss or the 
Kullback-Leibler one.

**** f1, f2, h1, h2
They take a float and give another float. They are defined according to the structure above, depending if string=L2 or string=KL.

**** inner constructor
Argument: a string
Output: if the string is "L2" or "KL", it defines the function fields according to the decomposition above, otherwise it raises an error.

*** Function GW_cost
Arguments: a Loss field, two metric measure spaces $M=(C,\mu)$ and $N=(D,\nu)$, a matrix of floats, a float $\varepsilon$.
Output: it computes the tensor product $E = L(C,D)\otimes T$ (it raises an error if the size are not compatible) and then it returns the
component-wise exponentiation of $E/\varepsilon$, so that the output is ready to be given in input as field K of a data_SK, so that it can be
used for the Sinkhorn algorithm.